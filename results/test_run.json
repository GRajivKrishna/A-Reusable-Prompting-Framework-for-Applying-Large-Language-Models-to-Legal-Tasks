{
  "benchmark_info": {
    "path": "benchmarks/legal_humaneval.jsonl",
    "total_samples": 2,
    "models_evaluated": [
      "mock-legal"
    ]
  },
  "model_results": {
    "mock-legal": {
      "role_based": {
        "pass_at_k": {
          "pass@1": 0.0
        },
        "aggregate_metrics": {
          "avg_citation_accuracy": 0.5,
          "avg_reasoning_coherence": 0.44999999999999996,
          "avg_completeness": 0.0,
          "avg_hallucination_rate": 0.0
        },
        "detailed_results": [
          {
            "citation_accuracy": 0.0,
            "reasoning_coherence": 0.3,
            "completeness": 0.0,
            "hallucination_rate": 0.0,
            "response_time": 0.0,
            "token_count": 0
          },
          {
            "citation_accuracy": 1.0,
            "reasoning_coherence": 0.6,
            "completeness": 0.0,
            "hallucination_rate": 0.0,
            "response_time": 0.0,
            "token_count": 0
          }
        ],
        "task_results": [
          "failed",
          "partial"
        ],
        "performance_metrics": {
          "total_tokens": 374.4,
          "avg_tokens_per_response": 187.2,
          "total_response_time": 1.001682996749878,
          "avg_response_time": 0.500841498374939,
          "error_rate": 0.0,
          "total_responses": 2
        }
      }
    }
  },
  "comparative_analysis": {
    "best_performing_models": {},
    "task_type_performance": {},
    "efficiency_metrics": {
      "mock-legal": {
        "avg_response_time": 0.500841498374939,
        "avg_tokens_per_response": 187.2,
        "error_rate": 0.0
      }
    }
  }
}