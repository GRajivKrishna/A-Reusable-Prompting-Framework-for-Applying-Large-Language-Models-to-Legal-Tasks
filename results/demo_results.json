{
  "benchmark_info": {
    "path": "benchmarks/legal_humaneval.jsonl",
    "total_samples": 5,
    "models_evaluated": [
      "mock-legal"
    ]
  },
  "model_results": {
    "mock-legal": {
      "role_based": {
        "pass_at_k": {
          "pass@1": 0.0,
          "pass@3": 0.0,
          "pass@5": 0.0
        },
        "aggregate_metrics": {
          "avg_citation_accuracy": 0.4,
          "avg_reasoning_coherence": 0.36,
          "avg_completeness": 0.0,
          "avg_hallucination_rate": 0.0
        },
        "detailed_results": [
          {
            "citation_accuracy": 0.0,
            "reasoning_coherence": 0.3,
            "completeness": 0.0,
            "hallucination_rate": 0.0,
            "response_time": 0.0,
            "token_count": 0
          },
          {
            "citation_accuracy": 1.0,
            "reasoning_coherence": 0.6,
            "completeness": 0.0,
            "hallucination_rate": 0.0,
            "response_time": 0.0,
            "token_count": 0
          },
          {
            "citation_accuracy": 0.0,
            "reasoning_coherence": 0.3,
            "completeness": 0.0,
            "hallucination_rate": 0.0,
            "response_time": 0.0,
            "token_count": 0
          },
          {
            "citation_accuracy": 1.0,
            "reasoning_coherence": 0.3,
            "completeness": 0.0,
            "hallucination_rate": 0.0,
            "response_time": 0.0,
            "token_count": 0
          },
          {
            "citation_accuracy": 0.0,
            "reasoning_coherence": 0.3,
            "completeness": 0.0,
            "hallucination_rate": 0.0,
            "response_time": 0.0,
            "token_count": 0
          }
        ],
        "task_results": [
          "failed",
          "partial",
          "failed",
          "partial",
          "failed"
        ],
        "performance_metrics": {
          "total_tokens": 937.3,
          "avg_tokens_per_response": 187.45999999999998,
          "total_response_time": 2.507267713546753,
          "avg_response_time": 0.5014535427093506,
          "error_rate": 0.0,
          "total_responses": 5
        }
      }
    }
  },
  "comparative_analysis": {
    "best_performing_models": {},
    "task_type_performance": {},
    "efficiency_metrics": {
      "mock-legal": {
        "avg_response_time": 0.5014535427093506,
        "avg_tokens_per_response": 187.45999999999998,
        "error_rate": 0.0
      }
    }
  }
}